{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be557a0f",
   "metadata": {},
   "source": [
    "### 1. GLUE dataset과 Huggingface\n",
    "    - Pretrained model의 성능을 측정하기 위해 최근은 SQuAD 등 기존에 유명한 데이터 셋 한 가지만을 갖고 비교했으나, 최근에는 다양한 task를 모델 하나만을 이용해 수행하여 비교하는 방식으로 바뀌었다.\n",
    "    - NLP 모델의 성능 측정에 대표적으로 활용되는 것이 GLUE(General Language Understanding Evaluation) Benchmark Dataset이다.\n",
    "\n",
    "> GLUE dataset은 다양한 자연어 처리(NLP) 태스크를 포함하고 있습니다. 각 태스크는 다음과 같은 내용을 다룹니다:\n",
    "\n",
    "1. **CoLA (Corpus of Linguistic Acceptability)**: 문법에 맞는 문장인지 판단합니다.\n",
    "\n",
    "2. **MNLI (Multi-Genre Natural Language Inference)**: 두 문장의 관계를 판단하며, entailment, contradiction, neutral 세 가지 범주로 분류합니다.\n",
    "\n",
    "3. **MNLI-MM (MNLI-Mismatched)**: MNLI와 유사하지만 두 문장이 안 맞는지를 판단합니다.\n",
    "\n",
    "4. **MRPC (Microsoft Research Paraphrase Corpus)**: 두 문장의 유사도를 평가합니다.\n",
    "\n",
    "5. **SST-2 (Stanford Sentiment Treebank)**: 감정 분석을 수행합니다.\n",
    "\n",
    "6. **STS-B (Semantic Textual Similarity Benchmark)**: 두 문장의 의미적 유사도를 평가합니다.\n",
    "\n",
    "7. **QQP (Quora Question Pairs)**: 두 질문의 유사도를 평가합니다.\n",
    "\n",
    "8. **QNLI (Question NLI)**: 질문과 문단 내 한 문장 간의 함의 관계를 판단합니다.\n",
    "\n",
    "9. **RTE (Recognizing Textual Entailment)**: 두 문장의 관계를 판단하며, entailment와 not_entailment 두 가지 범주로 분류합니다.\n",
    "\n",
    "10. **WNLI (Winograd Schema Challenge)**: 원문장과 대명사로 치환한 문장 사이의 함의 관계를 판단합니다.\n",
    "\n",
    "11. **Diagnostic Main**: 자연어 이해 능력을 평가하는 자연어 추론 문제를 다룹니다.\n",
    "\n",
    "이러한 태스크들은 자연어 처리 모델의 다양한 측면을 평가하고 향상시키는 데 사용됩니다. Huggingface를 통해 GLUE 데이터셋에 대한 접근이 간편해졌으며, 이를 통해 다양한 모델을 훈련하고 평가할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8886d",
   "metadata": {},
   "source": [
    "- 아래 코드가 잘 수행되지 않으면 본 명령어를 터미널에 입력합니다.\n",
    "```\n",
    "pip uninstall transformers -y\n",
    "pip install transformers\n",
    "mkdir -p transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddbabb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Downloading: 100%|██████████████████████████████| 629/629 [00:00<00:00, 499kB/s]\n",
      "Downloading: 100%|███████████████████████████| 255M/255M [00:03<00:00, 87.5MB/s]\n",
      "Downloading: 100%|███████████████████████████| 48.0/48.0 [00:00<00:00, 34.6kB/s]\n",
      "Downloading: 100%|███████████████████████████| 226k/226k [00:00<00:00, 6.64MB/s]\n",
      "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]\n"
     ]
    }
   ],
   "source": [
    "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849b683",
   "metadata": {},
   "source": [
    "### 2. 커스텀 프로젝트 제작 (1) Dataset\n",
    "    - 구슬이 서 말이라도 꿰어야 보배다! 데이터를 task에 맞게 가공해봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e5a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/aiffel/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60aded2d758c4621adfc5de0289ed1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_mrpc_dataset = load_dataset('glue', 'mrpc')\n",
    "print(huggingface_mrpc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3b5b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence1', 'sentence2', 'label', 'idx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_mrpc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0394ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 : Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "sentence2 : Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
      "label : 1\n",
      "idx : 0\n",
      "\n",
      "\n",
      "sentence1 : Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\n",
      "sentence2 : Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\n",
      "label : 0\n",
      "idx : 1\n",
      "\n",
      "\n",
      "sentence1 : They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .\n",
      "sentence2 : On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\n",
      "label : 1\n",
      "idx : 2\n",
      "\n",
      "\n",
      "sentence1 : Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .\n",
      "sentence2 : Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .\n",
      "label : 0\n",
      "idx : 3\n",
      "\n",
      "\n",
      "sentence1 : The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .\n",
      "sentence2 : PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .\n",
      "label : 1\n",
      "idx : 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d76db",
   "metadata": {},
   "source": [
    "#### 커스텀 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a671dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from datasets import Dataset\n",
    "\n",
    "tf_dataset, tf_dataset_info = tfds.load('glue/mrpc', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2186ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 : tf.Tensor(b'Shares in BA were down 1.5 percent at 168 pence by 1420 GMT , off a low of 164p , in a slightly stronger overall London market .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'Shares in BA were down three percent at 165-1 / 4 pence by 0933 GMT , off a low of 164 pence , in a stronger market .', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(163, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b'The South Korean Agriculture and Forestry Ministry also said it would throw out or send back all Canadian beef currently in store .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'The South Korean Agriculture and Forestry Ministry said it would scrap or return all Canadian beef in store .', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(131, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b'\" New Yorkers didn \\'t embrace these units like they could have , \" said Matthew Daus , chairman of the commission .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'\" New Yorkers didn \\'t embrace these units like they could have , \" Matthew W. Daus , the commission \\'s chairman , said yesterday .', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(1579, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b'\" I really liked him and I still do , \" Cohen Alon told the Herald yesterday .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'And I really liked him , and I still do .', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(1151, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b'Tight media controls and the remote location of the northern village where the fighting broke out made it impossible to confirm what happened .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'Tight media controls and the remote location of the clash made it impossible to confirm what happened .', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(1021, shape=(), dtype=int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = tf_dataset['test'].take(5)\n",
    "\n",
    "for example in examples:\n",
    "    for col in cols:\n",
    "        print(col, \":\", example[col])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2a5653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 : tf.Tensor(b'The identical rovers will act as robotic geologists , searching for evidence of past water .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'The rovers act as robotic geologists , moving on six wheels .', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(1680, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b\"Less than 20 percent of Boise 's sales would come from making lumber and paper after the OfficeMax purchase is completed .\", shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b\"Less than 20 percent of Boise 's sales would come from making lumber and paper after the OfficeMax purchase is complete , assuming those businesses aren 't sold .\", shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(1456, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b'Spider-Man snatched $ 114.7 million in its debut last year and went on to capture $ 403.7 million .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'Spider-Man , rated PG-13 , snatched $ 114.7 million in its first weekend and went on to take in $ 403.7 million .', shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(3017, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b\"The 2002 second quarter results don 't include figures from our friends at Compaq .\", shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'The year-ago numbers do not include figures from Compaq Computer .', shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(2896, shape=(), dtype=int32)\n",
      "\n",
      "\n",
      "sentence1 : tf.Tensor(b'Solomon 5.5 is available initially in the United States and Canada , for a starting price of about $ 12,700 .', shape=(), dtype=string)\n",
      "sentence2 : tf.Tensor(b'Solomon 5.5 is now available in the U.S. and Canada through Microsoft Business Solutions resellers .', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "idx : tf.Tensor(499, shape=(), dtype=int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = tf_dataset['train'].take(5)\n",
    "\n",
    "for example in examples:\n",
    "    for col in cols:\n",
    "        print(col, \":\", example[col])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90c2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow dataset 구조를 python dict 형식으로 변경\n",
    "# Dataset이 train, validation, test로 나뉘도록 구성\n",
    "train_dataset = tfds.as_dataframe(tf_dataset['train'], tf_dataset_info)\n",
    "val_dataset = tfds.as_dataframe(tf_dataset['validation'], tf_dataset_info)\n",
    "test_dataset = tfds.as_dataframe(tf_dataset['test'], tf_dataset_info)\n",
    "\n",
    "# dataframe 데이터를 dict 내부에 list로 변경\n",
    "train_dataset = train_dataset.to_dict('list')\n",
    "val_dataset = val_dataset.to_dict('list')\n",
    "test_dataset = test_dataset.to_dict('list')\n",
    "\n",
    "# Huggingface dataset\n",
    "tf_train_dataset = Dataset.from_dict(train_dataset)\n",
    "tf_val_dataset = Dataset.from_dict(val_dataset)\n",
    "tf_test_dataset = Dataset.from_dict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a380a6",
   "metadata": {},
   "source": [
    "### 3. 커스텀 프로젝트 제작 (2) Tokenizer와 Model\n",
    "    - Custom project를 위한 모델과 tokenizer을 불러와봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bdc7ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "161239cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['sentence1'],\n",
    "        data['sentence2'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d257fb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b55ecc4e9e4324b1521f8368112a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0889f661754aefa4f89ec541a9f3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ef8afb34c3414f8b899c5b5498b927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = huggingface_mrpc_dataset.map(transform, batched=True)\n",
    "\n",
    "# train & validation & test split\n",
    "hf_train_dataset = hf_dataset['train']\n",
    "hf_val_dataset = hf_dataset['validation']\n",
    "hf_test_dataset = hf_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2621aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bdd6c233704d92b1f6a5dcde43db71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e9b30eedbc49fa88500c96e5d79b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2010b1b87cf4c7981c0705fb6802661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_tf(batch):\n",
    "    sentence1 = [s.decode('utf-8') for s in batch['sentence1']]\n",
    "    sentence2 = [s.decode('utf-8') for s in batch['sentence2']]\n",
    "    return huggingface_tokenizer(\n",
    "        sentence1,\n",
    "        sentence2,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "# 토큰화 및 패딩을 적용\n",
    "tf_train_dataset = tf_train_dataset.map(transform_tf, batched=True)\n",
    "tf_val_dataset = tf_val_dataset.map(transform_tf, batched=True)\n",
    "tf_test_dataset = tf_test_dataset.map(transform_tf, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b70b0",
   "metadata": {},
   "source": [
    "### 4. 커스텀 프로젝트 제작 (3) Train/Evaluation과 Test\n",
    "    - Keras와 Huggingface 두 가지 방식으로 훈련해보아요.\n",
    "    \n",
    "#### 4-1. Trainer를 활용한 학습\n",
    "    - Hugging Face의 Trainer를 활용해 학습을 진행\n",
    "    - Trainer를 활용하기 위해선 Training Arguments를 통해 학습 관련 설정을 미리 지정해야 한다.\n",
    "    - 데이터 셋은 hf_train_dataset, hf_val_dataset, hf_test_dataset을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06cc5778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56ef77",
   "metadata": {},
   "source": [
    "- Trainer의 인자로 넘겨주어야 하는 것 중에 compute_metrics 메소드가 있다.\n",
    "- Task가 Classification인지 Regression인지에 따라 모델의 출력 형태가 달라지므로 **task별로 적합한 출력 형식을 고려해 모델의 성능을 계산하는 방법을 지정해준다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08500ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b46cb559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1377\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 09:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419615</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.890728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.503800</td>\n",
       "      <td>0.488906</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.322300</td>\n",
       "      <td>0.555631</td>\n",
       "      <td>0.830882</td>\n",
       "      <td>0.883642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.36206477434633433, metrics={'train_runtime': 572.0924, 'train_samples_per_second': 19.235, 'train_steps_per_second': 2.407, 'total_flos': 1457671254810624.0, 'train_loss': 0.36206477434633433, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a99300b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1725\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5840746164321899,\n",
       " 'eval_accuracy': 0.8260869565217391,\n",
       " 'eval_f1': 0.8739495798319329,\n",
       " 'eval_runtime': 28.6173,\n",
       " 'eval_samples_per_second': 60.278,\n",
       " 'eval_steps_per_second': 7.548,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c4ccc",
   "metadata": {},
   "source": [
    "#### 4-2. Custom Dataset으로 학습시켜보기\n",
    "    - tf_train_dataset, tf_val_dataset, tf_test_dataset으로도 학습을 진행해보고 결과를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf6b86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리를 비워줍니다.\n",
    "del huggingface_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "723db1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309651a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1377\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 09:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.511601</td>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.881410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.511100</td>\n",
       "      <td>0.399080</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.890034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>0.527691</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.893103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.35851471235376386, metrics={'train_runtime': 563.0815, 'train_samples_per_second': 19.542, 'train_steps_per_second': 2.445, 'total_flos': 1457671254810624.0, 'train_loss': 0.35851471235376386, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=tf_train_dataset,    # training dataset\n",
    "    eval_dataset=tf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
